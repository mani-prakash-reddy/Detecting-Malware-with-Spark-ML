from pyspark.ml.feature import NGram, CountVectorizer, VectorAssembler
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from collections import namedtuple
from pyspark.sql.functions import lit

def logistic_regression_model(inputCol="byte_content", N=2):ngrams = [NGram(n=N, inputCol="byte_content", outputCol="{0}_grams".format(N))];vectorizers = [CountVectorizer(inputCol="{0}_grams".format(N),outputCol="{0}_counts".format(N),vocabSize = 100)];assembler = [VectorAssembler(inputCols=["{0}_counts".format(N)],outputCol="features")];lr = [LogisticRegression(maxIter=100 )];return Pipeline(stages=ngrams + vectorizers + assembler+lr)

benign_rdd = sc.binaryFiles("/project/benign_win")
malicious_rdd = sc.binaryFiles("/project/malicious_win")
BYTES_UPPER_LIMIT = 5000
Sample = namedtuple("Sample", ["file_name", "byte_content"])

benign_df = benign_rdd.map(lambda rec: Sample(rec[0], [str(x) for x in bytes(rec[1])][0:BYTES_UPPER_LIMIT])).toDF()
malicious_df = malicious_rdd.map(lambda rec: Sample(rec[0], [str(x) for x in bytes(rec[1])][0:BYTES_UPPER_LIMIT])).toDF()
benign_df = benign_df.withColumn("label",lit(0))
malicious_df = malicious_df.withColumn("label",lit(1))
benign_df.show(5)
malicious_df.show(5)

df = benign_df.unionAll(malicious_df)

df_train, df_test = df.randomSplit([0.8,0.2])
logistic_regression = logistic_regression_model()
model = logistic_regression.fit(df_train)

df_pred = model.transform(df_test)

df_pred.show(5)

print(f"accuracy = {df_pred.filter(df_pred.label == df_pred.prediction).count() / float(df_pred.count())}")
